{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-09T09:22:33.000913Z",
     "iopub.status.busy": "2025-11-09T09:22:33.000737Z",
     "iopub.status.idle": "2025-11-09T09:22:34.553673Z",
     "shell.execute_reply": "2025-11-09T09:22:34.552925Z",
     "shell.execute_reply.started": "2025-11-09T09:22:33.000896Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T09:22:34.555372Z",
     "iopub.status.busy": "2025-11-09T09:22:34.554997Z",
     "iopub.status.idle": "2025-11-09T09:23:34.933913Z",
     "shell.execute_reply": "2025-11-09T09:23:34.933195Z",
     "shell.execute_reply.started": "2025-11-09T09:22:34.555354Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/Shannu3766/bi_influence.git\n",
      "  Cloning https://github.com/Shannu3766/bi_influence.git to /tmp/pip-req-build-ig8niloc\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/Shannu3766/bi_influence.git /tmp/pip-req-build-ig8niloc\n",
      "  Resolved https://github.com/Shannu3766/bi_influence.git to commit a1f735bcf7f738164d6a532874f7fa3bc62f216c\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from adaptive_lora==2.1.0) (2.6.0+cu124)\n",
      "Requirement already satisfied: transformers>=4.30 in /usr/local/lib/python3.11/dist-packages (from adaptive_lora==2.1.0) (4.53.3)\n",
      "Requirement already satisfied: peft>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from adaptive_lora==2.1.0) (0.16.0)\n",
      "Requirement already satisfied: datasets>=2.0 in /usr/local/lib/python3.11/dist-packages (from adaptive_lora==2.1.0) (4.4.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from adaptive_lora==2.1.0) (4.67.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from adaptive_lora==2.1.0) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from adaptive_lora==2.1.0) (1.2.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from adaptive_lora==2.1.0) (1.9.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from adaptive_lora==2.1.0) (2.2.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0->adaptive_lora==2.1.0) (3.20.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets>=2.0->adaptive_lora==2.1.0)\n",
      "  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0->adaptive_lora==2.1.0) (0.4.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0->adaptive_lora==2.1.0) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0->adaptive_lora==2.1.0) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0->adaptive_lora==2.1.0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0->adaptive_lora==2.1.0) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.0->adaptive_lora==2.1.0) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0->adaptive_lora==2.1.0) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0->adaptive_lora==2.1.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0->adaptive_lora==2.1.0) (6.0.3)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->adaptive_lora==2.1.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->adaptive_lora==2.1.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->adaptive_lora==2.1.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->adaptive_lora==2.1.0) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->adaptive_lora==2.1.0) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->adaptive_lora==2.1.0) (2.4.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft>=0.3.0->adaptive_lora==2.1.0) (7.1.3)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft>=0.3.0->adaptive_lora==2.1.0) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->adaptive_lora==2.1.0) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->adaptive_lora==2.1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->adaptive_lora==2.1.0) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13->adaptive_lora==2.1.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13->adaptive_lora==2.1.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13->adaptive_lora==2.1.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13->adaptive_lora==2.1.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13->adaptive_lora==2.1.0)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13->adaptive_lora==2.1.0)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13->adaptive_lora==2.1.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13->adaptive_lora==2.1.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13->adaptive_lora==2.1.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->adaptive_lora==2.1.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->adaptive_lora==2.1.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->adaptive_lora==2.1.0) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13->adaptive_lora==2.1.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->adaptive_lora==2.1.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->adaptive_lora==2.1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->adaptive_lora==2.1.0) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30->adaptive_lora==2.1.0) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30->adaptive_lora==2.1.0) (0.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->adaptive_lora==2.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->adaptive_lora==2.1.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->adaptive_lora==2.1.0) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->adaptive_lora==2.1.0) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->adaptive_lora==2.1.0) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->adaptive_lora==2.1.0) (3.6.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.0->adaptive_lora==2.1.0) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0->adaptive_lora==2.1.0) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0->adaptive_lora==2.1.0) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0->adaptive_lora==2.1.0) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0->adaptive_lora==2.1.0) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0->adaptive_lora==2.1.0) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=2.0->adaptive_lora==2.1.0) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->adaptive_lora==2.1.0) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.0->adaptive_lora==2.1.0) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.0->adaptive_lora==2.1.0) (2.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13->adaptive_lora==2.1.0) (3.0.3)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->adaptive_lora==2.1.0) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->adaptive_lora==2.1.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->adaptive_lora==2.1.0) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->adaptive_lora==2.1.0) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->adaptive_lora==2.1.0) (2024.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.0->adaptive_lora==2.1.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.0->adaptive_lora==2.1.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.0->adaptive_lora==2.1.0) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.0->adaptive_lora==2.1.0) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.0->adaptive_lora==2.1.0) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.0->adaptive_lora==2.1.0) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.0->adaptive_lora==2.1.0) (1.22.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->adaptive_lora==2.1.0) (2024.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets>=2.0->adaptive_lora==2.1.0) (1.3.1)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m345.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m328.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m330.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m339.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m282.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m320.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m329.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m317.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m287.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m326.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m304.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: adaptive_lora\n",
      "  Building wheel for adaptive_lora (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for adaptive_lora: filename=adaptive_lora-2.1.0-py3-none-any.whl size=9240 sha256=c0da25bbca934f80d96be7d0d8d8429139bb46671497de0fed1561864ca53df2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-taa_df_4/wheels/4b/91/1d/e153ff3aa3759d6ccdcc4706d8559f1cbfaa4aa22b992958cd\n",
      "Successfully built adaptive_lora\n",
      "Installing collected packages: pyarrow, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, adaptive_lora\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 19.0.1\n",
      "    Uninstalling pyarrow-19.0.1:\n",
      "      Successfully uninstalled pyarrow-19.0.1\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed adaptive_lora-2.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyarrow-22.0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "!pip install --upgrade --no-cache-dir git+https://github.com/Shannu3766/bi_influence.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T09:23:34.935211Z",
     "iopub.status.busy": "2025-11-09T09:23:34.934932Z",
     "iopub.status.idle": "2025-11-09T09:23:38.531232Z",
     "shell.execute_reply": "2025-11-09T09:23:38.530511Z",
     "shell.execute_reply.started": "2025-11-09T09:23:34.935187Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets evaluate accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T09:23:38.533048Z",
     "iopub.status.busy": "2025-11-09T09:23:38.532303Z",
     "iopub.status.idle": "2025-11-09T09:23:54.252430Z",
     "shell.execute_reply": "2025-11-09T09:23:54.251751Z",
     "shell.execute_reply.started": "2025-11-09T09:23:38.533018Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==2.20.0\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pyarrow==15.0.2\n",
      "  Downloading pyarrow-15.0.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting transformers==4.44.2\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting evaluate==0.4.2\n",
      "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (1.26.4)\n",
      "Collecting pyarrow-hotfix (from datasets==2.20.0)\n",
      "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.20.0)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (0.70.18)\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0)\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (3.13.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (2025.11.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (0.5.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.44.2)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (1.22.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.2->datasets==2.20.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.2->datasets==2.20.0) (1.2.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==2.20.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==2.20.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==2.20.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==2.20.0) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==2.20.0) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==2.20.0) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0) (2025.10.5)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.20.0)\n",
      "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0) (1.17.0)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets==2.20.0) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets==2.20.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets==2.20.0) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets==2.20.0) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets==2.20.0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets==2.20.0) (2024.2.0)\n",
      "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-15.0.2-cp311-cp311-manylinux_2_28_x86_64.whl (38.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m326.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m333.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m288.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m288.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m347.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m327.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m341.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
      "Installing collected packages: pyarrow-hotfix, fsspec, dill, multiprocess, tokenizers, pyarrow, datasets, transformers, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.10.0\n",
      "    Uninstalling fsspec-2025.10.0:\n",
      "      Successfully uninstalled fsspec-2025.10.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.4.0\n",
      "    Uninstalling dill-0.4.0:\n",
      "      Successfully uninstalled dill-0.4.0\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.18\n",
      "    Uninstalling multiprocess-0.70.18:\n",
      "      Successfully uninstalled multiprocess-0.70.18\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 22.0.0\n",
      "    Uninstalling pyarrow-22.0.0:\n",
      "      Successfully uninstalled pyarrow-22.0.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.4.1\n",
      "    Uninstalling datasets-4.4.1:\n",
      "      Successfully uninstalled datasets-4.4.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.53.3\n",
      "    Uninstalling transformers-4.53.3:\n",
      "      Successfully uninstalled transformers-4.53.3\n",
      "  Attempting uninstall: evaluate\n",
      "    Found existing installation: evaluate 0.4.6\n",
      "    Uninstalling evaluate-0.4.6:\n",
      "      Successfully uninstalled evaluate-0.4.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "s3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2024.5.0 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 evaluate-0.4.2 fsspec-2024.5.0 multiprocess-0.70.16 pyarrow-15.0.2 pyarrow-hotfix-0.7 tokenizers-0.19.1 transformers-4.44.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets==2.20.0 pyarrow==15.0.2 transformers==4.44.2 evaluate==0.4.2 --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T09:23:54.254212Z",
     "iopub.status.busy": "2025-11-09T09:23:54.253976Z",
     "iopub.status.idle": "2025-11-09T09:23:57.727989Z",
     "shell.execute_reply": "2025-11-09T09:23:57.727172Z",
     "shell.execute_reply.started": "2025-11-09T09:23:54.254188Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets evaluate accelerate scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T09:23:57.729250Z",
     "iopub.status.busy": "2025-11-09T09:23:57.729019Z",
     "iopub.status.idle": "2025-11-09T09:24:01.400232Z",
     "shell.execute_reply": "2025-11-09T09:24:01.399432Z",
     "shell.execute_reply.started": "2025-11-09T09:23:57.729225Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets evaluate accelerate scikit-learn pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T09:24:01.403282Z",
     "iopub.status.busy": "2025-11-09T09:24:01.403001Z",
     "iopub.status.idle": "2025-11-09T09:24:24.420233Z",
     "shell.execute_reply": "2025-11-09T09:24:24.419670Z",
     "shell.execute_reply.started": "2025-11-09T09:24:01.403256Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.Tensor size changed, may indicate binary incompatibility. Expected 64 from C header, got 80 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.ChunkedArray size changed, may indicate binary incompatibility. Expected 64 from C header, got 72 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib._Tabular size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.Table size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.Tensor size changed, may indicate binary incompatibility. Expected 64 from C header, got 80 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.ChunkedArray size changed, may indicate binary incompatibility. Expected 64 from C header, got 72 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib._Tabular size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.Table size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "2025-11-09 09:24:08.527137: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762680248.696222      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762680248.742535      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from torch.utils.data import DataLoader\n",
    "from adaptive_lora.callbacks import AdaptiveLoRACallback\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T09:24:24.421349Z",
     "iopub.status.busy": "2025-11-09T09:24:24.420887Z",
     "iopub.status.idle": "2025-11-09T09:24:27.476076Z",
     "shell.execute_reply": "2025-11-09T09:24:27.475520Z",
     "shell.execute_reply.started": "2025-11-09T09:24:24.421319Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5a218e58284f5c94abd5719dd261ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acbe79aff2654762ae2a602ec26ba6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad32735d5b14e07bacedb27b7aa24f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27528a21baa348a0a5c7f86fdf0a2cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ffcc53dc9d4af9a9f5168d51fdd75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ecea031fe744c90a6e6e2fba8c9315a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T09:24:27.477116Z",
     "iopub.status.busy": "2025-11-09T09:24:27.476852Z",
     "iopub.status.idle": "2025-11-09T09:24:28.387030Z",
     "shell.execute_reply": "2025-11-09T09:24:28.386371Z",
     "shell.execute_reply.started": "2025-11-09T09:24:27.477088Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 443,906 || all params: 124,500,484 || trainable%: 0.3565\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\", \"key\"],  # for QA models\n",
    "    task_type=TaskType.QUESTION_ANS,\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T09:24:28.388003Z",
     "iopub.status.busy": "2025-11-09T09:24:28.387785Z",
     "iopub.status.idle": "2025-11-09T09:24:31.089666Z",
     "shell.execute_reply": "2025-11-09T09:24:31.088934Z",
     "shell.execute_reply.started": "2025-11-09T09:24:28.387986Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb68b6877b442809a7d3717adfaf4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a399bab962b54d309396bce2fef3e330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/16.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2713f50c0db14e5dab21dda722cb22fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ee96ef88bf4f1497ca73449e2404cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cae86913be4482bc1d614b49d7cc55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"squad_v2\")\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"validation\"].select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T09:24:31.091014Z",
     "iopub.status.busy": "2025-11-09T09:24:31.090554Z",
     "iopub.status.idle": "2025-11-09T09:24:31.095764Z",
     "shell.execute_reply": "2025-11-09T09:24:31.095054Z",
     "shell.execute_reply.started": "2025-11-09T09:24:31.090994Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset),len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T09:24:31.096777Z",
     "iopub.status.busy": "2025-11-09T09:24:31.096526Z",
     "iopub.status.idle": "2025-11-09T09:24:31.496210Z",
     "shell.execute_reply": "2025-11-09T09:24:31.495529Z",
     "shell.execute_reply.started": "2025-11-09T09:24:31.096756Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771e4059ee5e445887558b4d53a0ac81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8b21ab1651421abe2a286db7f00db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 384\n",
    "doc_stride = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = inputs[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        sample_idx = sample_map[i]\n",
    "        answers = examples[\"answers\"][sample_idx]\n",
    "\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "            if not (\n",
    "                offsets[token_start_index][0] <= start_char\n",
    "                and offsets[token_end_index][1] >= end_char\n",
    "            ):\n",
    "                start_positions.append(cls_index)\n",
    "                end_positions.append(cls_index)\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                start_positions.append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                end_positions.append(token_end_index + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_eval = eval_dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"validation\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T09:24:31.497224Z",
     "iopub.status.busy": "2025-11-09T09:24:31.496960Z",
     "iopub.status.idle": "2025-11-09T09:24:31.501151Z",
     "shell.execute_reply": "2025-11-09T09:24:31.500526Z",
     "shell.execute_reply.started": "2025-11-09T09:24:31.497201Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_loader = DataLoader(\n",
    "    tokenized_eval,\n",
    "    batch_size=4,\n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "adaptive_callback = AdaptiveLoRACallback(\n",
    "    total_rank=512,\n",
    "    tau=0.4,\n",
    "    val_dataloader=val_loader,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T09:24:31.502428Z",
     "iopub.status.busy": "2025-11-09T09:24:31.501996Z",
     "iopub.status.idle": "2025-11-09T09:24:31.512000Z",
     "shell.execute_reply": "2025-11-09T09:24:31.511268Z",
     "shell.execute_reply.started": "2025-11-09T09:24:31.502409Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    start_logits, end_logits = predictions\n",
    "    start_positions, end_positions = labels\n",
    "\n",
    "\n",
    "    start_pred = np.argmax(start_logits, axis=1)\n",
    "    end_pred = np.argmax(end_logits, axis=1)\n",
    "\n",
    "    em = np.mean((start_pred == start_positions) & (end_pred == end_positions)) * 100\n",
    "\n",
    "    f1_scores = []\n",
    "    for s_pred, e_pred, s_gold, e_gold in zip(start_pred, end_pred, start_positions, end_positions):\n",
    "\n",
    "        pred_tokens = set(range(s_pred, e_pred + 1))\n",
    "        gold_tokens = set(range(s_gold, e_gold + 1))\n",
    "        common = len(pred_tokens & gold_tokens)\n",
    "\n",
    "        if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            precision = common / len(pred_tokens)\n",
    "            recall = common / len(gold_tokens)\n",
    "            f1 = (2 * precision * recall) / (precision + recall + 1e-8)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    f1 = np.mean(f1_scores) * 100\n",
    "    return {\"exact_match\": em, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T09:24:31.512949Z",
     "iopub.status.busy": "2025-11-09T09:24:31.512747Z",
     "iopub.status.idle": "2025-11-09T09:24:31.885495Z",
     "shell.execute_reply": "2025-11-09T09:24:31.884914Z",
     "shell.execute_reply.started": "2025-11-09T09:24:31.512926Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qa_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[adaptive_callback],\n",
    "    data_collator=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T09:24:31.886635Z",
     "iopub.status.busy": "2025-11-09T09:24:31.886355Z",
     "iopub.status.idle": "2025-11-09T09:24:31.890147Z",
     "shell.execute_reply": "2025-11-09T09:24:31.889512Z",
     "shell.execute_reply.started": "2025-11-09T09:24:31.886613Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # 9️⃣ Precompute Ranks Before Training\n",
    "# # ============================================================\n",
    "# print(\"\\nPrecomputing initial BI-based ranks before training...\")\n",
    "# adaptive_callback.on_epoch_begin(training_args, trainer.state, trainer.control, model)\n",
    "# print(\"✅ Initial ranks set based on BI importance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T09:24:31.891030Z",
     "iopub.status.busy": "2025-11-09T09:24:31.890803Z",
     "iopub.status.idle": "2025-11-09T09:24:37.342960Z",
     "shell.execute_reply": "2025-11-09T09:24:37.342234Z",
     "shell.execute_reply.started": "2025-11-09T09:24:31.891011Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting Adaptive LoRA QA Training...\n",
      "\n",
      "--- AdaptiveLoRA: Preparing ranks for Epoch 1 ---\n",
      "Computing BI importance scores (pre-training)...\n",
      "Allocating new ranks based on BI scores...\n",
      "Applying new ranks to LoRA modules for this epoch...\n",
      "  - base_model.model.roberta.encoder.layer.0.attention.self.query: r=8 → 18 (Score: 0.6894)\n",
      "  - base_model.model.roberta.encoder.layer.0.attention.self.key: r=8 → 23 (Score: 0.7802)\n",
      "  - base_model.model.roberta.encoder.layer.0.attention.self.value: r=8 → 14 (Score: 0.5676)\n",
      "  - base_model.model.roberta.encoder.layer.1.attention.self.query: r=8 → 23 (Score: 0.7716)\n",
      "  - base_model.model.roberta.encoder.layer.1.attention.self.key: r=8 → 11 (Score: 0.4678)\n",
      "  - base_model.model.roberta.encoder.layer.1.attention.self.value: r=8 → 16 (Score: 0.6229)\n",
      "  - base_model.model.roberta.encoder.layer.2.attention.self.query: r=8 → 27 (Score: 0.8491)\n",
      "  - base_model.model.roberta.encoder.layer.2.attention.self.key: r=8 → 3 (Score: 0.0000)\n",
      "  - base_model.model.roberta.encoder.layer.2.attention.self.value: r=8 → 9 (Score: 0.3890)\n",
      "  - base_model.model.roberta.encoder.layer.3.attention.self.query: r=8 (Unchanged, Score: 0.3452)\n",
      "  - base_model.model.roberta.encoder.layer.3.attention.self.key: r=8 → 17 (Score: 0.6615)\n",
      "  - base_model.model.roberta.encoder.layer.3.attention.self.value: r=8 → 40 (Score: 1.0000)\n",
      "  - base_model.model.roberta.encoder.layer.4.attention.self.query: r=8 → 16 (Score: 0.6212)\n",
      "  - base_model.model.roberta.encoder.layer.4.attention.self.key: r=8 → 10 (Score: 0.4271)\n",
      "  - base_model.model.roberta.encoder.layer.4.attention.self.value: r=8 → 12 (Score: 0.5259)\n",
      "  - base_model.model.roberta.encoder.layer.5.attention.self.query: r=8 → 10 (Score: 0.4534)\n",
      "  - base_model.model.roberta.encoder.layer.5.attention.self.key: r=8 → 17 (Score: 0.6593)\n",
      "  - base_model.model.roberta.encoder.layer.5.attention.self.value: r=8 → 6 (Score: 0.2708)\n",
      "  - base_model.model.roberta.encoder.layer.6.attention.self.query: r=8 → 7 (Score: 0.2936)\n",
      "  - base_model.model.roberta.encoder.layer.6.attention.self.key: r=8 → 5 (Score: 0.1897)\n",
      "  - base_model.model.roberta.encoder.layer.6.attention.self.value: r=8 → 26 (Score: 0.8196)\n",
      "  - base_model.model.roberta.encoder.layer.7.attention.self.query: r=8 → 25 (Score: 0.8065)\n",
      "  - base_model.model.roberta.encoder.layer.7.attention.self.key: r=8 → 11 (Score: 0.4944)\n",
      "  - base_model.model.roberta.encoder.layer.7.attention.self.value: r=8 → 9 (Score: 0.4206)\n",
      "  - base_model.model.roberta.encoder.layer.8.attention.self.query: r=8 → 6 (Score: 0.2568)\n",
      "  - base_model.model.roberta.encoder.layer.8.attention.self.key: r=8 → 6 (Score: 0.2402)\n",
      "  - base_model.model.roberta.encoder.layer.8.attention.self.value: r=8 → 6 (Score: 0.2448)\n",
      "  - base_model.model.roberta.encoder.layer.9.attention.self.query: r=8 → 11 (Score: 0.4858)\n",
      "  - base_model.model.roberta.encoder.layer.9.attention.self.key: r=8 → 22 (Score: 0.7618)\n",
      "  - base_model.model.roberta.encoder.layer.9.attention.self.value: r=8 → 9 (Score: 0.4172)\n",
      "  - base_model.model.roberta.encoder.layer.10.attention.self.query: r=8 → 15 (Score: 0.6140)\n",
      "  - base_model.model.roberta.encoder.layer.10.attention.self.key: r=8 → 17 (Score: 0.6513)\n",
      "  - base_model.model.roberta.encoder.layer.10.attention.self.value: r=8 → 15 (Score: 0.6009)\n",
      "  - base_model.model.roberta.encoder.layer.11.attention.self.query: r=8 → 10 (Score: 0.4245)\n",
      "  - base_model.model.roberta.encoder.layer.11.attention.self.key: r=8 → 12 (Score: 0.5132)\n",
      "  - base_model.model.roberta.encoder.layer.11.attention.self.value: r=8 → 20 (Score: 0.7217)\n",
      "✅ AdaptiveLoRA: Rank setup for Epoch 1 complete.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.253821</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.253736</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.253694</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Epoch 1: Rank allocations logged to ./logs/adaptive_lora_epoch_logs.csv\n",
      "\n",
      "\n",
      "--- AdaptiveLoRA: Preparing ranks for Epoch 2 ---\n",
      "Computing BI importance scores (pre-training)...\n",
      "Allocating new ranks based on BI scores...\n",
      "Applying new ranks to LoRA modules for this epoch...\n",
      "  - base_model.model.roberta.encoder.layer.0.attention.self.query: r=18 (Unchanged, Score: 0.6894)\n",
      "  - base_model.model.roberta.encoder.layer.0.attention.self.key: r=23 (Unchanged, Score: 0.7802)\n",
      "  - base_model.model.roberta.encoder.layer.0.attention.self.value: r=14 (Unchanged, Score: 0.5676)\n",
      "  - base_model.model.roberta.encoder.layer.1.attention.self.query: r=23 (Unchanged, Score: 0.7716)\n",
      "  - base_model.model.roberta.encoder.layer.1.attention.self.key: r=11 (Unchanged, Score: 0.4678)\n",
      "  - base_model.model.roberta.encoder.layer.1.attention.self.value: r=16 (Unchanged, Score: 0.6229)\n",
      "  - base_model.model.roberta.encoder.layer.2.attention.self.query: r=27 (Unchanged, Score: 0.8491)\n",
      "  - base_model.model.roberta.encoder.layer.2.attention.self.key: r=3 (Unchanged, Score: 0.0000)\n",
      "  - base_model.model.roberta.encoder.layer.2.attention.self.value: r=9 (Unchanged, Score: 0.3890)\n",
      "  - base_model.model.roberta.encoder.layer.3.attention.self.query: r=8 (Unchanged, Score: 0.3452)\n",
      "  - base_model.model.roberta.encoder.layer.3.attention.self.key: r=17 (Unchanged, Score: 0.6615)\n",
      "  - base_model.model.roberta.encoder.layer.3.attention.self.value: r=40 (Unchanged, Score: 1.0000)\n",
      "  - base_model.model.roberta.encoder.layer.4.attention.self.query: r=16 (Unchanged, Score: 0.6212)\n",
      "  - base_model.model.roberta.encoder.layer.4.attention.self.key: r=10 (Unchanged, Score: 0.4271)\n",
      "  - base_model.model.roberta.encoder.layer.4.attention.self.value: r=12 (Unchanged, Score: 0.5259)\n",
      "  - base_model.model.roberta.encoder.layer.5.attention.self.query: r=10 (Unchanged, Score: 0.4534)\n",
      "  - base_model.model.roberta.encoder.layer.5.attention.self.key: r=17 (Unchanged, Score: 0.6593)\n",
      "  - base_model.model.roberta.encoder.layer.5.attention.self.value: r=6 (Unchanged, Score: 0.2708)\n",
      "  - base_model.model.roberta.encoder.layer.6.attention.self.query: r=7 (Unchanged, Score: 0.2936)\n",
      "  - base_model.model.roberta.encoder.layer.6.attention.self.key: r=5 (Unchanged, Score: 0.1897)\n",
      "  - base_model.model.roberta.encoder.layer.6.attention.self.value: r=26 (Unchanged, Score: 0.8196)\n",
      "  - base_model.model.roberta.encoder.layer.7.attention.self.query: r=25 (Unchanged, Score: 0.8065)\n",
      "  - base_model.model.roberta.encoder.layer.7.attention.self.key: r=11 (Unchanged, Score: 0.4944)\n",
      "  - base_model.model.roberta.encoder.layer.7.attention.self.value: r=9 (Unchanged, Score: 0.4206)\n",
      "  - base_model.model.roberta.encoder.layer.8.attention.self.query: r=6 (Unchanged, Score: 0.2568)\n",
      "  - base_model.model.roberta.encoder.layer.8.attention.self.key: r=6 (Unchanged, Score: 0.2402)\n",
      "  - base_model.model.roberta.encoder.layer.8.attention.self.value: r=6 (Unchanged, Score: 0.2448)\n",
      "  - base_model.model.roberta.encoder.layer.9.attention.self.query: r=11 (Unchanged, Score: 0.4858)\n",
      "  - base_model.model.roberta.encoder.layer.9.attention.self.key: r=22 (Unchanged, Score: 0.7618)\n",
      "  - base_model.model.roberta.encoder.layer.9.attention.self.value: r=9 (Unchanged, Score: 0.4172)\n",
      "  - base_model.model.roberta.encoder.layer.10.attention.self.query: r=15 (Unchanged, Score: 0.6140)\n",
      "  - base_model.model.roberta.encoder.layer.10.attention.self.key: r=17 (Unchanged, Score: 0.6513)\n",
      "  - base_model.model.roberta.encoder.layer.10.attention.self.value: r=15 (Unchanged, Score: 0.6009)\n",
      "  - base_model.model.roberta.encoder.layer.11.attention.self.query: r=10 (Unchanged, Score: 0.4245)\n",
      "  - base_model.model.roberta.encoder.layer.11.attention.self.key: r=12 (Unchanged, Score: 0.5132)\n",
      "  - base_model.model.roberta.encoder.layer.11.attention.self.value: r=20 (Unchanged, Score: 0.7217)\n",
      "✅ AdaptiveLoRA: Rank setup for Epoch 2 complete.\n",
      "\n",
      "📄 Epoch 2: Rank allocations logged to ./logs/adaptive_lora_epoch_logs.csv\n",
      "\n",
      "\n",
      "--- AdaptiveLoRA: Preparing ranks for Epoch 3 ---\n",
      "Computing BI importance scores (pre-training)...\n",
      "Allocating new ranks based on BI scores...\n",
      "Applying new ranks to LoRA modules for this epoch...\n",
      "  - base_model.model.roberta.encoder.layer.0.attention.self.query: r=18 (Unchanged, Score: 0.6894)\n",
      "  - base_model.model.roberta.encoder.layer.0.attention.self.key: r=23 (Unchanged, Score: 0.7802)\n",
      "  - base_model.model.roberta.encoder.layer.0.attention.self.value: r=14 (Unchanged, Score: 0.5676)\n",
      "  - base_model.model.roberta.encoder.layer.1.attention.self.query: r=23 (Unchanged, Score: 0.7716)\n",
      "  - base_model.model.roberta.encoder.layer.1.attention.self.key: r=11 (Unchanged, Score: 0.4678)\n",
      "  - base_model.model.roberta.encoder.layer.1.attention.self.value: r=16 (Unchanged, Score: 0.6229)\n",
      "  - base_model.model.roberta.encoder.layer.2.attention.self.query: r=27 (Unchanged, Score: 0.8491)\n",
      "  - base_model.model.roberta.encoder.layer.2.attention.self.key: r=3 (Unchanged, Score: 0.0000)\n",
      "  - base_model.model.roberta.encoder.layer.2.attention.self.value: r=9 (Unchanged, Score: 0.3890)\n",
      "  - base_model.model.roberta.encoder.layer.3.attention.self.query: r=8 (Unchanged, Score: 0.3451)\n",
      "  - base_model.model.roberta.encoder.layer.3.attention.self.key: r=17 (Unchanged, Score: 0.6615)\n",
      "  - base_model.model.roberta.encoder.layer.3.attention.self.value: r=40 (Unchanged, Score: 1.0000)\n",
      "  - base_model.model.roberta.encoder.layer.4.attention.self.query: r=16 (Unchanged, Score: 0.6212)\n",
      "  - base_model.model.roberta.encoder.layer.4.attention.self.key: r=10 (Unchanged, Score: 0.4271)\n",
      "  - base_model.model.roberta.encoder.layer.4.attention.self.value: r=12 (Unchanged, Score: 0.5259)\n",
      "  - base_model.model.roberta.encoder.layer.5.attention.self.query: r=10 (Unchanged, Score: 0.4534)\n",
      "  - base_model.model.roberta.encoder.layer.5.attention.self.key: r=17 (Unchanged, Score: 0.6593)\n",
      "  - base_model.model.roberta.encoder.layer.5.attention.self.value: r=6 (Unchanged, Score: 0.2708)\n",
      "  - base_model.model.roberta.encoder.layer.6.attention.self.query: r=7 (Unchanged, Score: 0.2936)\n",
      "  - base_model.model.roberta.encoder.layer.6.attention.self.key: r=5 (Unchanged, Score: 0.1897)\n",
      "  - base_model.model.roberta.encoder.layer.6.attention.self.value: r=26 (Unchanged, Score: 0.8196)\n",
      "  - base_model.model.roberta.encoder.layer.7.attention.self.query: r=25 (Unchanged, Score: 0.8065)\n",
      "  - base_model.model.roberta.encoder.layer.7.attention.self.key: r=11 (Unchanged, Score: 0.4944)\n",
      "  - base_model.model.roberta.encoder.layer.7.attention.self.value: r=9 (Unchanged, Score: 0.4206)\n",
      "  - base_model.model.roberta.encoder.layer.8.attention.self.query: r=6 (Unchanged, Score: 0.2568)\n",
      "  - base_model.model.roberta.encoder.layer.8.attention.self.key: r=6 (Unchanged, Score: 0.2402)\n",
      "  - base_model.model.roberta.encoder.layer.8.attention.self.value: r=6 (Unchanged, Score: 0.2448)\n",
      "  - base_model.model.roberta.encoder.layer.9.attention.self.query: r=11 (Unchanged, Score: 0.4858)\n",
      "  - base_model.model.roberta.encoder.layer.9.attention.self.key: r=22 (Unchanged, Score: 0.7618)\n",
      "  - base_model.model.roberta.encoder.layer.9.attention.self.value: r=9 (Unchanged, Score: 0.4172)\n",
      "  - base_model.model.roberta.encoder.layer.10.attention.self.query: r=15 (Unchanged, Score: 0.6140)\n",
      "  - base_model.model.roberta.encoder.layer.10.attention.self.key: r=17 (Unchanged, Score: 0.6513)\n",
      "  - base_model.model.roberta.encoder.layer.10.attention.self.value: r=15 (Unchanged, Score: 0.6009)\n",
      "  - base_model.model.roberta.encoder.layer.11.attention.self.query: r=10 (Unchanged, Score: 0.4245)\n",
      "  - base_model.model.roberta.encoder.layer.11.attention.self.key: r=12 (Unchanged, Score: 0.5132)\n",
      "  - base_model.model.roberta.encoder.layer.11.attention.self.value: r=20 (Unchanged, Score: 0.7217)\n",
      "✅ AdaptiveLoRA: Rank setup for Epoch 3 complete.\n",
      "\n",
      "📄 Epoch 3: Rank allocations logged to ./logs/adaptive_lora_epoch_logs.csv\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.4580574035644531, metrics={'train_runtime': 5.0852, 'train_samples_per_second': 5.9, 'train_steps_per_second': 0.59, 'total_flos': 5935499458560.0, 'train_loss': 0.4580574035644531, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n🚀 Starting Adaptive LoRA QA Training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T09:24:37.344111Z",
     "iopub.status.busy": "2025-11-09T09:24:37.343753Z",
     "iopub.status.idle": "2025-11-09T09:24:37.519802Z",
     "shell.execute_reply": "2025-11-09T09:24:37.519140Z",
     "shell.execute_reply.started": "2025-11-09T09:24:37.344085Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Final Evaluation on Validation Split...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2536941170692444, 'eval_exact_match': 80.0, 'eval_f1': 87.99999955199999, 'eval_runtime': 0.1675, 'eval_samples_per_second': 59.71, 'eval_steps_per_second': 5.971, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🔍 Final Evaluation on Validation Split...\")\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T09:24:37.520800Z",
     "iopub.status.busy": "2025-11-09T09:24:37.520591Z",
     "iopub.status.idle": "2025-11-09T09:24:39.015689Z",
     "shell.execute_reply": "2025-11-09T09:24:39.014867Z",
     "shell.execute_reply.started": "2025-11-09T09:24:37.520785Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ Example Prediction:  Paris\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🧪 11️⃣ Example Prediction\n",
    "# ============================================================\n",
    "context = \"The Eiffel Tower is located in Paris and was completed in 1889.\"\n",
    "question = \"Where is the Eiffel Tower located?\"\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "start_idx = torch.argmax(outputs.start_logits)\n",
    "end_idx = torch.argmax(outputs.end_logits)\n",
    "answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx : end_idx + 1])\n",
    "\n",
    "print(f\"\\n✨ Example Prediction: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
